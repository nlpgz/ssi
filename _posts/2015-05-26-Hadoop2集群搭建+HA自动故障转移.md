---
layout: detail_tmp
title: Hadoop2集群搭建+HA自动故障转移
intro: Hadoop2集群搭建+HA自动故障转移
categories: Front-end development
keyword: Hadoop2集群搭建+HA自动故障转移
author: RandyChan
show_type: image
show_intro: /res/img/page/bigdata/h2_env.jpg
---


##在hadoop1节点先安装jdk##

	解压缩jdk
		tar -zxf jdk-8u45-linux-x64.tar -C /usr/local
	创建软连接
		ln -s /usr/local/jdk1.8.0_45 /usr/local/jdk
	设置环境变量
		vi /etc/profile
		export JAVA_HOME=/usr/local/jdk
		export PATH=.:$JAVA_HOME/bin:$PATH
	使环境变量生效
		source /etc/profile
	验证：
		java -version
		
**在hadoop1上执行命令**

        scp -rq /usr/local/jdk1.8.0_45 hadoop2:/usr/local
        scp -rq /usr/local/jdk1.8.0_45 hadoop3:/usr/local
        scp -rq /usr/local/jdk1.8.0_45 hadoop4:/usr/local
        scp -rq /usr/local/jdk1.8.0_45 hadoop5:/usr/local
        scp -rq /usr/local/jdk1.8.0_45 hadoop6:/usr/local
        scp -rq /etc/profile hadoop2:/etc
        scp -rq /etc/profile hadoop3:/etc
        scp -rq /etc/profile hadoop4:/etc
        scp -rq /etc/profile hadoop5:/etc
        scp -rq /etc/profile hadoop6:/etc
    在hadoop2、hadoop3、hadoop4、hadoop5、hadoop6执行命令
        ln -s /usr/local/jdk1.8.0_45 /usr/local/jdk
        source /etc/profile
        java -version
        
        
**(1) 各个节点所运行的角色如下**

        zookeeper：hadoop4、hadoop5、hadoop6
        namenode：hadoop1、hadoop2
        datanode：hadoop4、hadoop5、hadoop6
        journalnode：hadoop1、hadoop2、hadoop3
        resourcemanager：hadoop3
        nodemanager：hadoop4、hadoop5、hadoop6
**(2) 部署zookeeper集群**

	在hadoop1上解压缩zookeeper
		tar -zxf zookeeper-3.4.6.tar.gz -C /usr/local
	创建软连接
		ln -s /usr/local/zookeeper-3.4.6 /usr/local/zookeeper
	把zoo_sample.cfg重命名为zoo.cfg
		mv /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg
	修改配置文件
		vi /usr/local/zookeeper/conf/zoo.cfg
			dataDir=/usr/local/zookeeper/data
			server.4=hadoop4:2888:3888
			server.5=hadoop5:2888:3888
			server.6=hadoop6:2888:3888
	创建目录
		mkdir /usr/local/zookeeper/data
	写入文件
		echo 4 > /usr/local/zookeeper/data/myid
	复制zookeeper文件夹到hadoop4、hadoop5、hadoop6上
		scp -rq /usr/local/zookeeper-3.4.6 hadoop4:/usr/local
		scp -rq /usr/local/zookeeper-3.4.6 hadoop5:/usr/local
		scp -rq /usr/local/zookeeper-3.4.6 hadoop6:/usr/local
	在hadoop4上执行命令
		ln -s /usr/local/zookeeper-3.4.6 /usr/local/zookeeper
		echo 4 > /usr/local/zookeeper/data/myid
	在hadoop5上执行命令
		ln -s /usr/local/zookeeper-3.4.6 /usr/local/zookeeper
		echo 5 > /usr/local/zookeeper/data/myid
	在hadoop6上执行命令
		ln -s /usr/local/zookeeper-3.4.6 /usr/local/zookeeper
		echo 6 > /usr/local/zookeeper/data/myid
	在hadoop4、hadoop5、hadoop6上，分别执行命令/usr/local/zookeeper/bin/zkServer.sh start
	验证：
		/usr/local/zookeeper/bin/zkCli.sh 进入后执行ls /
		
**(3) 部署hadoop集群**
	
	在hadoop1解压缩hadoop
		tar -zxf hadoop-2.6.0.tar.gz -C /usr/local
	创建软连接
		ln -s /usr/local/hadoop-2.6.0 /usr/local/hadoop
	设置环境变量
		vi /etc/profile
		export HADOOP_HOME=/usr/local/hadoop
		export PATH=.:$JAVA_HOME/bin:$MAVEN_HOME/bin:$PROTOBUF_HOME/bin:$HADOOP_HOME/bin:$PATH
	使环境变量生效
		source /etc/profile
	配置文件(hadoop-env.sh、core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml、slaves)
	编辑文件
		vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh
		export JAVA_HOME=/usr/local/jdk
	编辑文件
		vi /usr/local/hadoop/etc/hadoop/core-site.xml
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://cluster1</value>
            </property>
            <property>
                <name>hadoop.tmp.dir</name>
                <value>/usr/local/hadoop/tmp</value>
            </property>
            <property>
                <name>ha.zookeeper.quorum</name>
                <value>hadoop4:2181,hadoop5:2181,hadoop6:2181</value>
            </property>
            <property>
                <name>fs.trash.interval</name>
                <value>1440</value>
            </property>
	编辑文件
		vi hdfs-site.xml
            <property>
                <name>dfs.replication</name>
                <value>3</value>
            </property>
            <property>
                <name>dfs.nameservices</name>
                <value>cluster1</value>
            </property>
            <property>
                <name>dfs.ha.namenodes.cluster1</name>
                <value>hadoop1,hadoop2</value>
            </property>
            <property>
                <name>dfs.namenode.rpc-address.cluster1.hadoop1</name>
                <value>hadoop1:9000</value>
            </property>
            <property>
                <name>dfs.namenode.http-address.cluster1.hadoop1</name>
                <value>hadoop1:50070</value>
            </property>
            <property>
                <name>dfs.namenode.rpc-address.cluster1.hadoop2</name>
                <value>hadoop2:9000</value>
            </property>
            <property>
                <name>dfs.namenode.http-address.cluster1.hadoop2</name>
                <value>hadoop2:50070</value>
            </property>
            <property>
                <name>dfs.ha.automatic-failover.enabled.cluster1</name>
                <value>true</value>
            </property>
            <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/cluster1</value>
            </property>
            <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/usr/local/hadoop/tmp/journal</value>
            </property>
            <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
            </property>
            <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/root/.ssh/id_rsa</value>
            </property>
            <property>
                <name>dfs.client.failover.proxy.provider.cluster1</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
            </property>
	编辑文件
		vi yarn-site.xml
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hadoop3</value>
            </property>
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>
	编辑文件
		vi mapred-site.xml
            <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
            </property>
	编辑文件
		vi slaves 增加内容如下
            hadoop4
            hadoop5
            hadoop6
	把hadoop1上的hadoop文件夹复制到hadoop2、hadoop3、hadoop4、hadoop5、hadoop6节点
		scp -rq /usr/local/hadoop-2.6.0 hadoop2:/usr/local
		scp -rq /usr/local/hadoop-2.6.0 hadoop3:/usr/local
		scp -rq /usr/local/hadoop-2.6.0 hadoop4:/usr/local
		scp -rq /usr/local/hadoop-2.6.0 hadoop5:/usr/local
		scp -rq /usr/local/hadoop-2.6.0 hadoop6:/usr/local
		scp -rq /etc/profile hadoop2:/etc
		scp -rq /etc/profile hadoop3:/etc
		scp -rq /etc/profile hadoop4:/etc
		scp -rq /etc/profile hadoop5:/etc
		scp -rq /etc/profile hadoop6:/etc
	在hadoop2、hadoop3、hadoop4、hadoop5、hadoop6执行命令
		ln -s /usr/local/hadoop-2.6.0 /usr/local/hadoop
		source /etc/profile
	格式化zk集群
		在hadoop1上执行/usr/local/hadoop/bin/hdfs zkfc -formatZK
	启动journalnode集群
		在hadoop1、hadoop2、hadoop3上别分执行/usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode
	格式化namenode、启动namenode
		在hadoop1上执行/usr/local/hadoop/bin/hdfs namenode -format
		在hadoop1上执行/usr/local/hadoop/sbin/hadoop-daemon.sh start namenode
		在hadoop2上执行/usr/local/hadoop/bin/hdfs namenode -bootstrapStandby
		在hadoop2上执行/usr/local/hadoop/sbin/hadoop-daemon.sh start namenode
	启动datanode
		在hadoop1上执行/usr/local/hadoop/sbin/hadoop-daemons.sh start datanode
	启动ZKFC
		在hadoop1、hadoop2分别执行命令/usr/local/hadoop/sbin/hadoop-daemon.sh start zkfc
	启动resourcemanager和nodemanager
		在hadoop3上执行/usr/local/hadoop/sbin/start-yarn.sh
	jps验证：
		hadoop1和hadoop2执行jsp显示进程有
			JournalNode、NameNode、DFSZKFailoverController
		hadoop3执行jsp显示进程有
			ResourceManager、JournalNode
		hadoop4、hadoop5、hadoop6执行jsp显示进程有
			DataNode、NodeManager、QuorumPeerMain
	通过浏览器验证
		在宿主机访问http://hadoop1:50070和http://hadoop2:50070 发现只有一个active和一个standby
		在宿主机访问http://hadoop3:8088 查看yarn的运行情况
	启动historyserver
		在任意一个节点启动即可，也可以在多个节点启动。执行命令/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver
	jps验证：
		执行jps查看JobHistoryServer进程是否存在
		通过浏览器验证：在宿主机访问http://historyserver_ip:19888